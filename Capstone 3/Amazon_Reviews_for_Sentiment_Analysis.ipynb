{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "BWqtug-_bwIK",
   "metadata": {
    "id": "BWqtug-_bwIK"
   },
   "source": [
    "Package and file imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6P1uT6a23czb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6P1uT6a23czb",
    "outputId": "d17539b8-916e-4209-8495-2d3f98cd5ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.11.2-py2.py3-none-any.whl (2.3 MB)\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "Requirement already satisfied: Pillow in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (9.2.0)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Using cached google_api_python_client-2.70.0-py2.py3-none-any.whl (10.7 MB)\n",
      "Collecting immutabledict\n",
      "  Using cached immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Using cached kaggle-1.5.12.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (3.6.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (1.24.0)\n",
      "Collecting oauth2client\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.6.0.66-cp37-abi3-macosx_11_0_arm64.whl (30.0 MB)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (1.5.2)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (5.9.4)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml<6.0,>=5.1\n",
      "  Using cached PyYAML-5.4.1.tar.gz (175 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (1.9.3)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97.tar.gz (524 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting seqeval\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (1.16.0)\n",
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.11.0-py2.py3-none-any.whl (2.3 MB)\n",
      "  Using cached tf_models_official-2.10.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Collecting sacrebleu==2.2.0\n",
      "  Using cached sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.10.0-py2.py3-none-any.whl (2.2 MB)\n",
      "  Using cached tf_models_official-2.9.2-py2.py3-none-any.whl (2.1 MB)\n",
      "  Using cached tf_models_official-2.9.1-py2.py3-none-any.whl (2.1 MB)\n",
      "  Using cached tf_models_official-2.9.0-py2.py3-none-any.whl (2.0 MB)\n",
      "  Using cached tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
      "  Using cached tf_models_official-2.7.2-py2.py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/evanmeeks/miniconda3/envs/sklearn-env/lib/python3.11/site-packages (from tf-models-official) (6.0)\n",
      "  Using cached tf_models_official-2.7.1-py2.py3-none-any.whl (1.8 MB)\n",
      "  Using cached tf_models_official-2.7.0-py2.py3-none-any.whl (1.8 MB)\n",
      "  Using cached tf_models_official-2.6.1-py2.py3-none-any.whl (1.8 MB)\n",
      "  Using cached tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
      "  Using cached tf_models_official-2.5.1-py2.py3-none-any.whl (1.6 MB)\n",
      "  Using cached tf_models_official-2.5.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting google-cloud-bigquery>=0.31.0\n",
      "  Using cached google_cloud_bigquery-3.4.1-py2.py3-none-any.whl (215 kB)\n",
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.4.0-py2.py3-none-any.whl (1.1 MB)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.3.0-py2.py3-none-any.whl (840 kB)\n",
      "  Using cached tf_models_official-2.2.2-py2.py3-none-any.whl (711 kB)\n",
      "Collecting mlperf-compliance==0.0.10\n",
      "  Using cached mlperf_compliance-0.0.10-py3-none-any.whl (24 kB)\n",
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.2.1-py2.py3-none-any.whl (711 kB)\n",
      "  Using cached tf_models_official-2.2.0-py2.py3-none-any.whl (711 kB)\n",
      "\u001b[31mERROR: Cannot install tf-models-official==2.10.0, tf-models-official==2.10.1, tf-models-official==2.11.0, tf-models-official==2.11.2, tf-models-official==2.2.0, tf-models-official==2.2.1, tf-models-official==2.2.2, tf-models-official==2.3.0, tf-models-official==2.4.0, tf-models-official==2.5.0, tf-models-official==2.5.1, tf-models-official==2.6.0, tf-models-official==2.6.1, tf-models-official==2.7.0, tf-models-official==2.7.1, tf-models-official==2.7.2, tf-models-official==2.8.0, tf-models-official==2.9.0, tf-models-official==2.9.1 and tf-models-official==2.9.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    tf-models-official 2.11.2 depends on tensorflow-addons\n",
      "    tf-models-official 2.11.0 depends on opencv-python-headless==4.5.2.52\n",
      "    tf-models-official 2.10.1 depends on tensorflow-addons\n",
      "    tf-models-official 2.10.0 depends on opencv-python-headless==4.5.2.52\n",
      "    tf-models-official 2.9.2 depends on tensorflow-addons\n",
      "    tf-models-official 2.9.1 depends on tensorflow-addons\n",
      "    tf-models-official 2.9.0 depends on tensorflow-addons\n",
      "    tf-models-official 2.8.0 depends on tensorflow-addons\n",
      "    tf-models-official 2.7.2 depends on tensorflow-addons\n",
      "    tf-models-official 2.7.1 depends on tensorflow-addons\n",
      "    tf-models-official 2.7.0 depends on tensorflow-addons\n",
      "    tf-models-official 2.6.1 depends on tensorflow-addons\n",
      "    tf-models-official 2.6.0 depends on tensorflow-addons\n",
      "    tf-models-official 2.5.1 depends on tensorflow-addons\n",
      "    tf-models-official 2.5.0 depends on tensorflow-addons\n",
      "    tf-models-official 2.4.0 depends on tensorflow-addons\n",
      "    tf-models-official 2.3.0 depends on tensorflow-addons\n",
      "    tf-models-official 2.2.2 depends on tensorflow-addons\n",
      "    tf-models-official 2.2.1 depends on tensorflow-addons\n",
      "    tf-models-official 2.2.0 depends on tensorflow-addons\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-text (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-text\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tf-models-official \n",
    "!pip install -U tensorflow\n",
    "!pip install -U tensorflow-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a25e18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7a25e18",
    "outputId": "cb593f0c-28ee-45c9-9121-4d0618f99ed4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evanmeeks/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/evanmeeks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import bz2\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shutil\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cceea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f6cceea",
    "outputId": "4f9aad33-22aa-445d-fcce-0bef12c9a137"
   },
   "outputs": [],
   "source": [
    "#Wrapping entire file import process in an if-else statement to speed up processing if the cleaned files have already\n",
    "# been exported\n",
    "\n",
    "if not os.path.exists('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_test_text.csv') and not os.path.exists('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_train_text.csv'):\n",
    "\n",
    "  #Reading in BZ2 Files\n",
    "  train_file = bz2.BZ2File('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/train.ft.txt.bz2')\n",
    "  test_file = bz2.BZ2File('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/test.ft.txt.bz2')\n",
    "\n",
    "  #Splitting files into their individual lines\n",
    "  train_file_lines = train_file.readlines()\n",
    "  test_file_lines = test_file.readlines()\n",
    "\n",
    "  #Decoding the lines into parseable data\n",
    "  train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "  test_file_lines = [x.decode('utf-8') for x in test_file_lines]\n",
    "\n",
    "  #Function to convert the individual lines into array objects. Note: \n",
    "  #Label 0 = negative, Label 1 = positive review. \n",
    "\n",
    "  def load_extract(file):\n",
    "    texts, labels = [], []\n",
    "    for line in file: \n",
    "      labels.append(int(line[9])-1)\n",
    "      texts.append(line[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "\n",
    "  #Calling the function on the data\n",
    "  train_labels, train_text = load_extract(train_file_lines)\n",
    "  test_labels, test_text = load_extract(test_file_lines)\n",
    "\n",
    "  #Taking only 10% of datasets because sheer size keeps crashing the notebook.\n",
    "  sub_train_text = train_text[:360000]\n",
    "  sub_train_labels = train_labels[:360000]\n",
    "  sub_test_text = test_text[:360000]\n",
    "  sub_test_labels = test_labels[:360000]\n",
    "\n",
    "  #Quick check that everything above worked smoothly.\n",
    "  print(sub_train_text[0])\n",
    "  print(sub_train_labels[0], \"\\n\")\n",
    "  print(sub_test_text[0])\n",
    "  print(sub_test_labels[0])\n",
    "\n",
    "\n",
    "else: \n",
    "\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe6e9b",
   "metadata": {
    "id": "e4fe6e9b"
   },
   "outputs": [],
   "source": [
    "#Function to clean the data of stop words, URLs/Hyperlinks, numbers, whitespace, etc. \n",
    "def clean_texts(texts):\n",
    "    stwords = stopwords.words('english')\n",
    "    temp_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        text = re.sub('\\d','0',texts[i])\n",
    "        if 'www.' in text or 'http:' in text or 'https:' in text or '.com' in text: \n",
    "            text = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \" \", text)\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = text.split()\n",
    "        text = [word for word in text if not word in stwords]\n",
    "        text = ' '.join(text)\n",
    "        temp_texts.append(text)\n",
    "    return temp_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VnrfepTvcTH-",
   "metadata": {
    "id": "VnrfepTvcTH-"
   },
   "source": [
    "Cleaning the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AGq9O93wjwPh",
   "metadata": {
    "id": "AGq9O93wjwPh"
   },
   "outputs": [],
   "source": [
    "#Custom CSV file open/reader\n",
    "def open_csv(file):\n",
    "  with open(file, mode='r') as csv_file:\n",
    "    result = []\n",
    "    for line in csv_file: \n",
    "      result.append(line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8ebfc",
   "metadata": {
    "id": "49b8ebfc"
   },
   "outputs": [],
   "source": [
    "#Creating the train/test text sets if they haven't already been exported\n",
    "\n",
    "if os.path.exists('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_train_text.csv'):\n",
    "\n",
    "  train_texts = open_csv('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_train_text.csv')\n",
    "  sub_train_labels = open_csv('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/sub_train_labels.csv')\n",
    "\n",
    "else: \n",
    "  \n",
    "    train_texts = clean_texts(sub_train_text)\n",
    "    np.savetxt('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_train_text.csv', train_texts, fmt ='%s')\n",
    "    np.savetxt('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/sub_train_labels.csv', sub_train_labels, fmt ='%s')\n",
    "\n",
    "\n",
    "if os.path.exists('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_test_text.csv'):\n",
    "\n",
    "  test_texts = open_csv('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_test_text.csv')\n",
    "  sub_train_labels = open_csv('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/sub_test_labels.csv')\n",
    "\n",
    "else: \n",
    "\n",
    "    test_texts = clean_texts(sub_test_text)\n",
    "    np.savetxt('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/cleaned_test_text.csv', test_texts, fmt ='%s')\n",
    "    np.savetxt('/Users/evanmeeks/Documents/data_education/Springboard/Github Projects/Capstone 3/sub_test_labels.csv', sub_test_labels, fmt ='%s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QQja6vDycbbA",
   "metadata": {
    "id": "QQja6vDycbbA"
   },
   "source": [
    "**Data Modeling using a Multinomial and Logitistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa86980",
   "metadata": {
    "id": "aaa86980"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(train_texts)\n",
    "\n",
    "train_texts_vec = count_vect.transform(train_texts)\n",
    "\n",
    "test_texts_vec = count_vect.transform(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09197b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a09197b",
    "outputId": "6b99776d-b3bf-4eed-faa7-7458412a3d4b"
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_texts_vec, sub_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576716ae",
   "metadata": {
    "id": "576716ae"
   },
   "outputs": [],
   "source": [
    "y_pred = nb.predict(test_texts_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184b77b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0184b77b",
    "outputId": "8fef820c-05bc-4a20-8520-6d117a4b94cb"
   },
   "outputs": [],
   "source": [
    "print('Accuracy:', accuracy_score(sub_test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba9072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeba9072",
    "outputId": "98f10bc3-b91c-4806-b356-1bfde94a6ba8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(n_jobs=-1, max_iter=150)\n",
    "lr_model.fit(train_texts_vec, sub_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a24496",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36a24496",
    "outputId": "ea53c9ad-523d-4677-adf8-4b08c56766e1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_lr = lr_model.predict(test_texts_vec)\n",
    "print('Accuracy:', accuracy_score(sub_test_labels, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ca6b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "040ca6b7",
    "outputId": "25a95ace-4c20-4bf1-e938-4c5a348d8192"
   },
   "outputs": [],
   "source": [
    "a = 1201\n",
    "sample = test_texts[a]\n",
    "print(sample)\n",
    "\n",
    "sample_vec = count_vect.transform([sample])\n",
    "pred = lr_model.predict(sample_vec)\n",
    "print('\\npredicted label:',pred[0])\n",
    "print('actual label:', test_labels[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rWvGEfcLcxpI",
   "metadata": {
    "id": "rWvGEfcLcxpI"
   },
   "source": [
    "**Data Modeling using Tensorflow's BERT (Bidirectional Encoder Representations from Transformers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jvLFzYoa6Neu",
   "metadata": {
    "id": "jvLFzYoa6Neu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, sub_train_labels, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pA4DZR5gjuaS",
   "metadata": {
    "id": "pA4DZR5gjuaS"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Reading csv file into TensorFlow dataset and printing first element for verification\n",
    "train_ds = tf.data.TextLineDataset([\"cleaned_train_text.csv\"])\n",
    "test_ds = tf.data.TextLineDataset([\"cleaned_test_text.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B4S93iO1wvCP",
   "metadata": {
    "id": "B4S93iO1wvCP"
   },
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JSTElixAxVfO",
   "metadata": {
    "id": "JSTElixAxVfO"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentences):\n",
    "    preprocessed_text = bert_preprocess(sentences)\n",
    "    return bert_encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LnS16XKv21ni",
   "metadata": {
    "id": "LnS16XKv21ni"
   },
   "outputs": [],
   "source": [
    "x=get_sentence_embedding([train_texts])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9yuk1zK2yOp0",
   "metadata": {
    "id": "9yuk1zK2yOp0"
   },
   "outputs": [],
   "source": [
    "# Bert layers\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Neural network layers\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# Use inputs and outputs to construct a final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RTjFNrCi5pul",
   "metadata": {
    "id": "RTjFNrCi5pul"
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V_bkaqUF588y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_bkaqUF588y",
    "outputId": "8bcd024e-e571-4603-fdd6-a57c57a401c0"
   },
   "outputs": [],
   "source": [
    "model.fit(np.array(X_train), y_train, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
